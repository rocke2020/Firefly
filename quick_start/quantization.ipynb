{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/ll/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 36.36s/it]\n",
      "2023-11-09 07:21:25 INFO [auto_gptq.modeling._base] Start quantizing layer 1/32\n",
      "2023-11-09 07:21:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 1/32...\n",
      "2023-11-09 07:21:28 INFO [auto_gptq.quantization.gptq] duration: 1.7066783905029297\n",
      "2023-11-09 07:21:28 INFO [auto_gptq.quantization.gptq] avg loss: 3.834463119506836\n",
      "2023-11-09 07:21:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 1/32...\n",
      "2023-11-09 07:21:30 INFO [auto_gptq.quantization.gptq] duration: 1.2426633834838867\n",
      "2023-11-09 07:21:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.20677444338798523\n",
      "2023-11-09 07:21:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 1/32...\n",
      "2023-11-09 07:21:31 INFO [auto_gptq.quantization.gptq] duration: 1.235335350036621\n",
      "2023-11-09 07:21:31 INFO [auto_gptq.quantization.gptq] avg loss: 4.246157646179199\n",
      "2023-11-09 07:21:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 1/32...\n",
      "2023-11-09 07:21:32 INFO [auto_gptq.quantization.gptq] duration: 1.217362880706787\n",
      "2023-11-09 07:21:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.0028959158807992935\n",
      "2023-11-09 07:21:32 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 1/32...\n",
      "2023-11-09 07:21:33 INFO [auto_gptq.quantization.gptq] duration: 1.246647834777832\n",
      "2023-11-09 07:21:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.16716237366199493\n",
      "2023-11-09 07:21:33 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 1/32...\n",
      "2023-11-09 07:21:35 INFO [auto_gptq.quantization.gptq] duration: 1.2504312992095947\n",
      "2023-11-09 07:21:35 INFO [auto_gptq.quantization.gptq] avg loss: 0.17523598670959473\n",
      "2023-11-09 07:21:35 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 1/32...\n",
      "2023-11-09 07:21:38 INFO [auto_gptq.quantization.gptq] duration: 3.5665788650512695\n",
      "2023-11-09 07:21:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.007681966759264469\n",
      "2023-11-09 07:21:38 INFO [auto_gptq.modeling._base] Start quantizing layer 2/32\n",
      "2023-11-09 07:21:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 2/32...\n",
      "2023-11-09 07:21:40 INFO [auto_gptq.quantization.gptq] duration: 1.2396125793457031\n",
      "2023-11-09 07:21:40 INFO [auto_gptq.quantization.gptq] avg loss: 1.6871631145477295\n",
      "2023-11-09 07:21:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 2/32...\n",
      "2023-11-09 07:21:41 INFO [auto_gptq.quantization.gptq] duration: 1.2285890579223633\n",
      "2023-11-09 07:21:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.19292595982551575\n",
      "2023-11-09 07:21:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 2/32...\n",
      "2023-11-09 07:21:42 INFO [auto_gptq.quantization.gptq] duration: 1.2374486923217773\n",
      "2023-11-09 07:21:42 INFO [auto_gptq.quantization.gptq] avg loss: 1.64994215965271\n",
      "2023-11-09 07:21:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 2/32...\n",
      "2023-11-09 07:21:44 INFO [auto_gptq.quantization.gptq] duration: 1.247743844985962\n",
      "2023-11-09 07:21:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.00332687608897686\n",
      "2023-11-09 07:21:44 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 2/32...\n",
      "2023-11-09 07:21:45 INFO [auto_gptq.quantization.gptq] duration: 1.2897522449493408\n",
      "2023-11-09 07:21:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.42610085010528564\n",
      "2023-11-09 07:21:45 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 2/32...\n",
      "2023-11-09 07:21:46 INFO [auto_gptq.quantization.gptq] duration: 1.3091166019439697\n",
      "2023-11-09 07:21:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.4880759119987488\n",
      "2023-11-09 07:21:46 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 2/32...\n",
      "2023-11-09 07:21:50 INFO [auto_gptq.quantization.gptq] duration: 3.5968680381774902\n",
      "2023-11-09 07:21:50 INFO [auto_gptq.quantization.gptq] avg loss: 2093.853271484375\n",
      "2023-11-09 07:21:50 INFO [auto_gptq.modeling._base] Start quantizing layer 3/32\n",
      "2023-11-09 07:21:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 3/32...\n",
      "2023-11-09 07:21:52 INFO [auto_gptq.quantization.gptq] duration: 1.2543365955352783\n",
      "2023-11-09 07:21:52 INFO [auto_gptq.quantization.gptq] avg loss: 3.2953715324401855\n",
      "2023-11-09 07:21:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 3/32...\n",
      "2023-11-09 07:21:53 INFO [auto_gptq.quantization.gptq] duration: 1.2604303359985352\n",
      "2023-11-09 07:21:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.7826151251792908\n",
      "2023-11-09 07:21:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 3/32...\n",
      "2023-11-09 07:21:54 INFO [auto_gptq.quantization.gptq] duration: 1.308929443359375\n",
      "2023-11-09 07:21:54 INFO [auto_gptq.quantization.gptq] avg loss: 3.062058448791504\n",
      "2023-11-09 07:21:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 3/32...\n",
      "2023-11-09 07:21:56 INFO [auto_gptq.quantization.gptq] duration: 1.2691588401794434\n",
      "2023-11-09 07:21:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.0064318180084228516\n",
      "2023-11-09 07:21:56 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 3/32...\n",
      "2023-11-09 07:21:57 INFO [auto_gptq.quantization.gptq] duration: 1.2970035076141357\n",
      "2023-11-09 07:21:57 INFO [auto_gptq.quantization.gptq] avg loss: 1.1043829917907715\n",
      "2023-11-09 07:21:57 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 3/32...\n",
      "2023-11-09 07:21:58 INFO [auto_gptq.quantization.gptq] duration: 1.2697093486785889\n",
      "2023-11-09 07:21:58 INFO [auto_gptq.quantization.gptq] avg loss: 1.286522626876831\n",
      "2023-11-09 07:21:58 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 3/32...\n",
      "2023-11-09 07:22:02 INFO [auto_gptq.quantization.gptq] duration: 3.6228854656219482\n",
      "2023-11-09 07:22:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.023880593478679657\n",
      "2023-11-09 07:22:02 INFO [auto_gptq.modeling._base] Start quantizing layer 4/32\n",
      "2023-11-09 07:22:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 4/32...\n",
      "2023-11-09 07:22:04 INFO [auto_gptq.quantization.gptq] duration: 1.2593295574188232\n",
      "2023-11-09 07:22:04 INFO [auto_gptq.quantization.gptq] avg loss: 5.823651313781738\n",
      "2023-11-09 07:22:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 4/32...\n",
      "2023-11-09 07:22:05 INFO [auto_gptq.quantization.gptq] duration: 1.2805988788604736\n",
      "2023-11-09 07:22:05 INFO [auto_gptq.quantization.gptq] avg loss: 1.4888849258422852\n",
      "2023-11-09 07:22:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 4/32...\n",
      "2023-11-09 07:22:06 INFO [auto_gptq.quantization.gptq] duration: 1.2390258312225342\n",
      "2023-11-09 07:22:06 INFO [auto_gptq.quantization.gptq] avg loss: 5.650704383850098\n",
      "2023-11-09 07:22:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 4/32...\n",
      "2023-11-09 07:22:08 INFO [auto_gptq.quantization.gptq] duration: 1.2368066310882568\n",
      "2023-11-09 07:22:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.0054696789011359215\n",
      "2023-11-09 07:22:08 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 4/32...\n",
      "2023-11-09 07:22:09 INFO [auto_gptq.quantization.gptq] duration: 1.2693629264831543\n",
      "2023-11-09 07:22:09 INFO [auto_gptq.quantization.gptq] avg loss: 1.4478883743286133\n",
      "2023-11-09 07:22:09 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 4/32...\n",
      "2023-11-09 07:22:10 INFO [auto_gptq.quantization.gptq] duration: 1.2677042484283447\n",
      "2023-11-09 07:22:10 INFO [auto_gptq.quantization.gptq] avg loss: 1.6945977210998535\n",
      "2023-11-09 07:22:10 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 4/32...\n",
      "2023-11-09 07:22:14 INFO [auto_gptq.quantization.gptq] duration: 3.568711519241333\n",
      "2023-11-09 07:22:14 INFO [auto_gptq.quantization.gptq] avg loss: 0.02198348380625248\n",
      "2023-11-09 07:22:14 INFO [auto_gptq.modeling._base] Start quantizing layer 5/32\n",
      "2023-11-09 07:22:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 5/32...\n",
      "2023-11-09 07:22:16 INFO [auto_gptq.quantization.gptq] duration: 1.2279484272003174\n",
      "2023-11-09 07:22:16 INFO [auto_gptq.quantization.gptq] avg loss: 7.091480731964111\n",
      "2023-11-09 07:22:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 5/32...\n",
      "2023-11-09 07:22:17 INFO [auto_gptq.quantization.gptq] duration: 1.224872350692749\n",
      "2023-11-09 07:22:17 INFO [auto_gptq.quantization.gptq] avg loss: 1.8510665893554688\n",
      "2023-11-09 07:22:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 5/32...\n",
      "2023-11-09 07:22:18 INFO [auto_gptq.quantization.gptq] duration: 1.2603795528411865\n",
      "2023-11-09 07:22:18 INFO [auto_gptq.quantization.gptq] avg loss: 6.934191703796387\n",
      "2023-11-09 07:22:18 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 5/32...\n",
      "2023-11-09 07:22:19 INFO [auto_gptq.quantization.gptq] duration: 1.2662336826324463\n",
      "2023-11-09 07:22:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.008002331480383873\n",
      "2023-11-09 07:22:19 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 5/32...\n",
      "2023-11-09 07:22:21 INFO [auto_gptq.quantization.gptq] duration: 1.2897045612335205\n",
      "2023-11-09 07:22:21 INFO [auto_gptq.quantization.gptq] avg loss: 2.2640867233276367\n",
      "2023-11-09 07:22:21 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 5/32...\n",
      "2023-11-09 07:22:22 INFO [auto_gptq.quantization.gptq] duration: 1.2906665802001953\n",
      "2023-11-09 07:22:22 INFO [auto_gptq.quantization.gptq] avg loss: 2.8278279304504395\n",
      "2023-11-09 07:22:22 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 5/32...\n",
      "2023-11-09 07:22:26 INFO [auto_gptq.quantization.gptq] duration: 3.692641019821167\n",
      "2023-11-09 07:22:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.08464319258928299\n",
      "2023-11-09 07:22:26 INFO [auto_gptq.modeling._base] Start quantizing layer 6/32\n",
      "2023-11-09 07:22:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 6/32...\n",
      "2023-11-09 07:22:28 INFO [auto_gptq.quantization.gptq] duration: 1.3028321266174316\n",
      "2023-11-09 07:22:28 INFO [auto_gptq.quantization.gptq] avg loss: 6.31348180770874\n",
      "2023-11-09 07:22:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 6/32...\n",
      "2023-11-09 07:22:29 INFO [auto_gptq.quantization.gptq] duration: 1.340083122253418\n",
      "2023-11-09 07:22:29 INFO [auto_gptq.quantization.gptq] avg loss: 1.6794471740722656\n",
      "2023-11-09 07:22:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 6/32...\n",
      "2023-11-09 07:22:30 INFO [auto_gptq.quantization.gptq] duration: 1.271615982055664\n",
      "2023-11-09 07:22:30 INFO [auto_gptq.quantization.gptq] avg loss: 5.883275985717773\n",
      "2023-11-09 07:22:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 6/32...\n",
      "2023-11-09 07:22:31 INFO [auto_gptq.quantization.gptq] duration: 1.2611851692199707\n",
      "2023-11-09 07:22:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.024210277944803238\n",
      "2023-11-09 07:22:31 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 6/32...\n",
      "2023-11-09 07:22:33 INFO [auto_gptq.quantization.gptq] duration: 1.2788186073303223\n",
      "2023-11-09 07:22:33 INFO [auto_gptq.quantization.gptq] avg loss: 2.22737979888916\n",
      "2023-11-09 07:22:33 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 6/32...\n",
      "2023-11-09 07:22:34 INFO [auto_gptq.quantization.gptq] duration: 1.2947311401367188\n",
      "2023-11-09 07:22:34 INFO [auto_gptq.quantization.gptq] avg loss: 2.788391590118408\n",
      "2023-11-09 07:22:34 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 6/32...\n",
      "2023-11-09 07:22:38 INFO [auto_gptq.quantization.gptq] duration: 3.6039464473724365\n",
      "2023-11-09 07:22:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.05374506860971451\n",
      "2023-11-09 07:22:38 INFO [auto_gptq.modeling._base] Start quantizing layer 7/32\n",
      "2023-11-09 07:22:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 7/32...\n",
      "2023-11-09 07:22:40 INFO [auto_gptq.quantization.gptq] duration: 1.2414569854736328\n",
      "2023-11-09 07:22:40 INFO [auto_gptq.quantization.gptq] avg loss: 8.464584350585938\n",
      "2023-11-09 07:22:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 7/32...\n",
      "2023-11-09 07:22:41 INFO [auto_gptq.quantization.gptq] duration: 1.2421326637268066\n",
      "2023-11-09 07:22:41 INFO [auto_gptq.quantization.gptq] avg loss: 2.3480286598205566\n",
      "2023-11-09 07:22:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 7/32...\n",
      "2023-11-09 07:22:42 INFO [auto_gptq.quantization.gptq] duration: 1.2379097938537598\n",
      "2023-11-09 07:22:42 INFO [auto_gptq.quantization.gptq] avg loss: 8.504377365112305\n",
      "2023-11-09 07:22:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 7/32...\n",
      "2023-11-09 07:22:43 INFO [auto_gptq.quantization.gptq] duration: 1.2260947227478027\n",
      "2023-11-09 07:22:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.030429305508732796\n",
      "2023-11-09 07:22:43 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 7/32...\n",
      "2023-11-09 07:22:45 INFO [auto_gptq.quantization.gptq] duration: 1.2593789100646973\n",
      "2023-11-09 07:22:45 INFO [auto_gptq.quantization.gptq] avg loss: 2.6679019927978516\n",
      "2023-11-09 07:22:45 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 7/32...\n",
      "2023-11-09 07:22:46 INFO [auto_gptq.quantization.gptq] duration: 1.252995491027832\n",
      "2023-11-09 07:22:46 INFO [auto_gptq.quantization.gptq] avg loss: 3.4817562103271484\n",
      "2023-11-09 07:22:46 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 7/32...\n",
      "2023-11-09 07:22:49 INFO [auto_gptq.quantization.gptq] duration: 3.548588514328003\n",
      "2023-11-09 07:22:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.08779048919677734\n",
      "2023-11-09 07:22:50 INFO [auto_gptq.modeling._base] Start quantizing layer 8/32\n",
      "2023-11-09 07:22:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 8/32...\n",
      "2023-11-09 07:22:51 INFO [auto_gptq.quantization.gptq] duration: 1.2377405166625977\n",
      "2023-11-09 07:22:51 INFO [auto_gptq.quantization.gptq] avg loss: 8.630237579345703\n",
      "2023-11-09 07:22:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 8/32...\n",
      "2023-11-09 07:22:52 INFO [auto_gptq.quantization.gptq] duration: 1.233184814453125\n",
      "2023-11-09 07:22:52 INFO [auto_gptq.quantization.gptq] avg loss: 2.5539755821228027\n",
      "2023-11-09 07:22:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 8/32...\n",
      "2023-11-09 07:22:54 INFO [auto_gptq.quantization.gptq] duration: 1.240199089050293\n",
      "2023-11-09 07:22:54 INFO [auto_gptq.quantization.gptq] avg loss: 8.754227638244629\n",
      "2023-11-09 07:22:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 8/32...\n",
      "2023-11-09 07:22:55 INFO [auto_gptq.quantization.gptq] duration: 1.2354142665863037\n",
      "2023-11-09 07:22:55 INFO [auto_gptq.quantization.gptq] avg loss: 0.03487830609083176\n",
      "2023-11-09 07:22:55 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 8/32...\n",
      "2023-11-09 07:22:56 INFO [auto_gptq.quantization.gptq] duration: 1.2655255794525146\n",
      "2023-11-09 07:22:56 INFO [auto_gptq.quantization.gptq] avg loss: 3.053546905517578\n",
      "2023-11-09 07:22:56 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 8/32...\n",
      "2023-11-09 07:22:58 INFO [auto_gptq.quantization.gptq] duration: 1.289093255996704\n",
      "2023-11-09 07:22:58 INFO [auto_gptq.quantization.gptq] avg loss: 3.957376480102539\n",
      "2023-11-09 07:22:58 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 8/32...\n",
      "2023-11-09 07:23:01 INFO [auto_gptq.quantization.gptq] duration: 3.569131851196289\n",
      "2023-11-09 07:23:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.11519328504800797\n",
      "2023-11-09 07:23:01 INFO [auto_gptq.modeling._base] Start quantizing layer 9/32\n",
      "2023-11-09 07:23:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 9/32...\n",
      "2023-11-09 07:23:03 INFO [auto_gptq.quantization.gptq] duration: 1.242023229598999\n",
      "2023-11-09 07:23:03 INFO [auto_gptq.quantization.gptq] avg loss: 8.663619995117188\n",
      "2023-11-09 07:23:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 9/32...\n",
      "2023-11-09 07:23:04 INFO [auto_gptq.quantization.gptq] duration: 1.2325375080108643\n",
      "2023-11-09 07:23:04 INFO [auto_gptq.quantization.gptq] avg loss: 2.6072239875793457\n",
      "2023-11-09 07:23:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 9/32...\n",
      "2023-11-09 07:23:05 INFO [auto_gptq.quantization.gptq] duration: 1.2300214767456055\n",
      "2023-11-09 07:23:05 INFO [auto_gptq.quantization.gptq] avg loss: 8.65134334564209\n",
      "2023-11-09 07:23:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 9/32...\n",
      "2023-11-09 07:23:07 INFO [auto_gptq.quantization.gptq] duration: 1.2345528602600098\n",
      "2023-11-09 07:23:07 INFO [auto_gptq.quantization.gptq] avg loss: 0.04395759850740433\n",
      "2023-11-09 07:23:07 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 9/32...\n",
      "2023-11-09 07:23:08 INFO [auto_gptq.quantization.gptq] duration: 1.2705373764038086\n",
      "2023-11-09 07:23:08 INFO [auto_gptq.quantization.gptq] avg loss: 3.298600912094116\n",
      "2023-11-09 07:23:08 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 9/32...\n",
      "2023-11-09 07:23:09 INFO [auto_gptq.quantization.gptq] duration: 1.2750368118286133\n",
      "2023-11-09 07:23:09 INFO [auto_gptq.quantization.gptq] avg loss: 4.027137756347656\n",
      "2023-11-09 07:23:09 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 9/32...\n",
      "2023-11-09 07:23:13 INFO [auto_gptq.quantization.gptq] duration: 3.553602933883667\n",
      "2023-11-09 07:23:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.15971285104751587\n",
      "2023-11-09 07:23:13 INFO [auto_gptq.modeling._base] Start quantizing layer 10/32\n",
      "2023-11-09 07:23:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 10/32...\n",
      "2023-11-09 07:23:15 INFO [auto_gptq.quantization.gptq] duration: 1.2246530055999756\n",
      "2023-11-09 07:23:15 INFO [auto_gptq.quantization.gptq] avg loss: 8.911395072937012\n",
      "2023-11-09 07:23:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 10/32...\n",
      "2023-11-09 07:23:16 INFO [auto_gptq.quantization.gptq] duration: 1.2339613437652588\n",
      "2023-11-09 07:23:16 INFO [auto_gptq.quantization.gptq] avg loss: 2.704362630844116\n",
      "2023-11-09 07:23:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 10/32...\n",
      "2023-11-09 07:23:17 INFO [auto_gptq.quantization.gptq] duration: 1.216294527053833\n",
      "2023-11-09 07:23:17 INFO [auto_gptq.quantization.gptq] avg loss: 8.544654846191406\n",
      "2023-11-09 07:23:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 10/32...\n",
      "2023-11-09 07:23:18 INFO [auto_gptq.quantization.gptq] duration: 1.2125415802001953\n",
      "2023-11-09 07:23:18 INFO [auto_gptq.quantization.gptq] avg loss: 0.0756000429391861\n",
      "2023-11-09 07:23:18 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 10/32...\n",
      "2023-11-09 07:23:20 INFO [auto_gptq.quantization.gptq] duration: 1.2469196319580078\n",
      "2023-11-09 07:23:20 INFO [auto_gptq.quantization.gptq] avg loss: 3.5651655197143555\n",
      "2023-11-09 07:23:20 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 10/32...\n",
      "2023-11-09 07:23:21 INFO [auto_gptq.quantization.gptq] duration: 1.2487080097198486\n",
      "2023-11-09 07:23:21 INFO [auto_gptq.quantization.gptq] avg loss: 4.221141338348389\n",
      "2023-11-09 07:23:21 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 10/32...\n",
      "2023-11-09 07:23:24 INFO [auto_gptq.quantization.gptq] duration: 3.523376941680908\n",
      "2023-11-09 07:23:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.1646571159362793\n",
      "2023-11-09 07:23:25 INFO [auto_gptq.modeling._base] Start quantizing layer 11/32\n",
      "2023-11-09 07:23:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 11/32...\n",
      "2023-11-09 07:23:26 INFO [auto_gptq.quantization.gptq] duration: 1.2812895774841309\n",
      "2023-11-09 07:23:26 INFO [auto_gptq.quantization.gptq] avg loss: 9.498499870300293\n",
      "2023-11-09 07:23:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 11/32...\n",
      "2023-11-09 07:23:27 INFO [auto_gptq.quantization.gptq] duration: 1.213136911392212\n",
      "2023-11-09 07:23:27 INFO [auto_gptq.quantization.gptq] avg loss: 2.806461811065674\n",
      "2023-11-09 07:23:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 11/32...\n",
      "2023-11-09 07:23:29 INFO [auto_gptq.quantization.gptq] duration: 1.2089025974273682\n",
      "2023-11-09 07:23:29 INFO [auto_gptq.quantization.gptq] avg loss: 8.920524597167969\n",
      "2023-11-09 07:23:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 11/32...\n",
      "2023-11-09 07:23:30 INFO [auto_gptq.quantization.gptq] duration: 1.206993818283081\n",
      "2023-11-09 07:23:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.07066020369529724\n",
      "2023-11-09 07:23:30 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 11/32...\n",
      "2023-11-09 07:23:31 INFO [auto_gptq.quantization.gptq] duration: 1.237494707107544\n",
      "2023-11-09 07:23:31 INFO [auto_gptq.quantization.gptq] avg loss: 3.818052291870117\n",
      "2023-11-09 07:23:31 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 11/32...\n",
      "2023-11-09 07:23:32 INFO [auto_gptq.quantization.gptq] duration: 1.2341034412384033\n",
      "2023-11-09 07:23:32 INFO [auto_gptq.quantization.gptq] avg loss: 4.412663459777832\n",
      "2023-11-09 07:23:32 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 11/32...\n",
      "2023-11-09 07:23:36 INFO [auto_gptq.quantization.gptq] duration: 3.4892172813415527\n",
      "2023-11-09 07:23:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.16832256317138672\n",
      "2023-11-09 07:23:36 INFO [auto_gptq.modeling._base] Start quantizing layer 12/32\n",
      "2023-11-09 07:23:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 12/32...\n",
      "2023-11-09 07:23:38 INFO [auto_gptq.quantization.gptq] duration: 1.2185287475585938\n",
      "2023-11-09 07:23:38 INFO [auto_gptq.quantization.gptq] avg loss: 9.809133529663086\n",
      "2023-11-09 07:23:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 12/32...\n",
      "2023-11-09 07:23:39 INFO [auto_gptq.quantization.gptq] duration: 1.282355785369873\n",
      "2023-11-09 07:23:39 INFO [auto_gptq.quantization.gptq] avg loss: 3.705920457839966\n",
      "2023-11-09 07:23:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 12/32...\n",
      "2023-11-09 07:23:40 INFO [auto_gptq.quantization.gptq] duration: 1.2209863662719727\n",
      "2023-11-09 07:23:40 INFO [auto_gptq.quantization.gptq] avg loss: 9.826007843017578\n",
      "2023-11-09 07:23:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 12/32...\n",
      "2023-11-09 07:23:41 INFO [auto_gptq.quantization.gptq] duration: 1.2418863773345947\n",
      "2023-11-09 07:23:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.0759262889623642\n",
      "2023-11-09 07:23:41 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 12/32...\n",
      "2023-11-09 07:23:43 INFO [auto_gptq.quantization.gptq] duration: 1.2965781688690186\n",
      "2023-11-09 07:23:43 INFO [auto_gptq.quantization.gptq] avg loss: 4.159365653991699\n",
      "2023-11-09 07:23:43 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 12/32...\n",
      "2023-11-09 07:23:44 INFO [auto_gptq.quantization.gptq] duration: 1.2755944728851318\n",
      "2023-11-09 07:23:44 INFO [auto_gptq.quantization.gptq] avg loss: 4.694455623626709\n",
      "2023-11-09 07:23:44 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 12/32...\n",
      "2023-11-09 07:23:48 INFO [auto_gptq.quantization.gptq] duration: 3.594478130340576\n",
      "2023-11-09 07:23:48 INFO [auto_gptq.quantization.gptq] avg loss: 0.18329675495624542\n",
      "2023-11-09 07:23:48 INFO [auto_gptq.modeling._base] Start quantizing layer 13/32\n",
      "2023-11-09 07:23:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 13/32...\n",
      "2023-11-09 07:23:50 INFO [auto_gptq.quantization.gptq] duration: 1.2195382118225098\n",
      "2023-11-09 07:23:50 INFO [auto_gptq.quantization.gptq] avg loss: 11.113357543945312\n",
      "2023-11-09 07:23:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 13/32...\n",
      "2023-11-09 07:23:51 INFO [auto_gptq.quantization.gptq] duration: 1.2164204120635986\n",
      "2023-11-09 07:23:51 INFO [auto_gptq.quantization.gptq] avg loss: 3.692841053009033\n",
      "2023-11-09 07:23:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 13/32...\n",
      "2023-11-09 07:23:52 INFO [auto_gptq.quantization.gptq] duration: 1.2139432430267334\n",
      "2023-11-09 07:23:52 INFO [auto_gptq.quantization.gptq] avg loss: 10.46481990814209\n",
      "2023-11-09 07:23:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 13/32...\n",
      "2023-11-09 07:23:53 INFO [auto_gptq.quantization.gptq] duration: 1.2205002307891846\n",
      "2023-11-09 07:23:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.08954542875289917\n",
      "2023-11-09 07:23:53 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 13/32...\n",
      "2023-11-09 07:23:55 INFO [auto_gptq.quantization.gptq] duration: 1.2500317096710205\n",
      "2023-11-09 07:23:55 INFO [auto_gptq.quantization.gptq] avg loss: 4.472163200378418\n",
      "2023-11-09 07:23:55 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 13/32...\n",
      "2023-11-09 07:23:56 INFO [auto_gptq.quantization.gptq] duration: 1.252018690109253\n",
      "2023-11-09 07:23:56 INFO [auto_gptq.quantization.gptq] avg loss: 4.88682746887207\n",
      "2023-11-09 07:23:56 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 13/32...\n",
      "2023-11-09 07:23:59 INFO [auto_gptq.quantization.gptq] duration: 3.529315233230591\n",
      "2023-11-09 07:23:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.18199953436851501\n",
      "2023-11-09 07:24:00 INFO [auto_gptq.modeling._base] Start quantizing layer 14/32\n",
      "2023-11-09 07:24:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 14/32...\n",
      "2023-11-09 07:24:01 INFO [auto_gptq.quantization.gptq] duration: 1.2321200370788574\n",
      "2023-11-09 07:24:01 INFO [auto_gptq.quantization.gptq] avg loss: 10.010098457336426\n",
      "2023-11-09 07:24:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 14/32...\n",
      "2023-11-09 07:24:02 INFO [auto_gptq.quantization.gptq] duration: 1.2111947536468506\n",
      "2023-11-09 07:24:02 INFO [auto_gptq.quantization.gptq] avg loss: 3.7468948364257812\n",
      "2023-11-09 07:24:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 14/32...\n",
      "2023-11-09 07:24:04 INFO [auto_gptq.quantization.gptq] duration: 1.210881233215332\n",
      "2023-11-09 07:24:04 INFO [auto_gptq.quantization.gptq] avg loss: 9.786585807800293\n",
      "2023-11-09 07:24:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 14/32...\n",
      "2023-11-09 07:24:05 INFO [auto_gptq.quantization.gptq] duration: 1.22943115234375\n",
      "2023-11-09 07:24:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.09418442100286484\n",
      "2023-11-09 07:24:05 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 14/32...\n",
      "2023-11-09 07:24:06 INFO [auto_gptq.quantization.gptq] duration: 1.2751190662384033\n",
      "2023-11-09 07:24:06 INFO [auto_gptq.quantization.gptq] avg loss: 4.649338722229004\n",
      "2023-11-09 07:24:06 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 14/32...\n",
      "2023-11-09 07:24:07 INFO [auto_gptq.quantization.gptq] duration: 1.2598333358764648\n",
      "2023-11-09 07:24:07 INFO [auto_gptq.quantization.gptq] avg loss: 4.939054012298584\n",
      "2023-11-09 07:24:07 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 14/32...\n",
      "2023-11-09 07:24:11 INFO [auto_gptq.quantization.gptq] duration: 3.7508795261383057\n",
      "2023-11-09 07:24:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.22146214544773102\n",
      "2023-11-09 07:24:11 INFO [auto_gptq.modeling._base] Start quantizing layer 15/32\n",
      "2023-11-09 07:24:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 15/32...\n",
      "2023-11-09 07:24:14 INFO [auto_gptq.quantization.gptq] duration: 2.0428943634033203\n",
      "2023-11-09 07:24:14 INFO [auto_gptq.quantization.gptq] avg loss: 10.761804580688477\n",
      "2023-11-09 07:24:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 15/32...\n",
      "2023-11-09 07:24:16 INFO [auto_gptq.quantization.gptq] duration: 2.051692008972168\n",
      "2023-11-09 07:24:16 INFO [auto_gptq.quantization.gptq] avg loss: 3.8354854583740234\n",
      "2023-11-09 07:24:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 15/32...\n",
      "2023-11-09 07:24:17 INFO [auto_gptq.quantization.gptq] duration: 1.3175902366638184\n",
      "2023-11-09 07:24:17 INFO [auto_gptq.quantization.gptq] avg loss: 10.191093444824219\n",
      "2023-11-09 07:24:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 15/32...\n",
      "2023-11-09 07:24:19 INFO [auto_gptq.quantization.gptq] duration: 1.3155725002288818\n",
      "2023-11-09 07:24:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.0941813737154007\n",
      "2023-11-09 07:24:19 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 15/32...\n",
      "2023-11-09 07:24:20 INFO [auto_gptq.quantization.gptq] duration: 1.3381493091583252\n",
      "2023-11-09 07:24:20 INFO [auto_gptq.quantization.gptq] avg loss: 4.945056915283203\n",
      "2023-11-09 07:24:20 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 15/32...\n",
      "2023-11-09 07:24:22 INFO [auto_gptq.quantization.gptq] duration: 1.727630853652954\n",
      "2023-11-09 07:24:22 INFO [auto_gptq.quantization.gptq] avg loss: 5.209650039672852\n",
      "2023-11-09 07:24:22 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 15/32...\n",
      "2023-11-09 07:24:26 INFO [auto_gptq.quantization.gptq] duration: 4.36974835395813\n",
      "2023-11-09 07:24:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.23964117467403412\n",
      "2023-11-09 07:24:26 INFO [auto_gptq.modeling._base] Start quantizing layer 16/32\n",
      "2023-11-09 07:24:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 16/32...\n",
      "2023-11-09 07:24:28 INFO [auto_gptq.quantization.gptq] duration: 1.5620818138122559\n",
      "2023-11-09 07:24:28 INFO [auto_gptq.quantization.gptq] avg loss: 10.749885559082031\n",
      "2023-11-09 07:24:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 16/32...\n",
      "2023-11-09 07:24:31 INFO [auto_gptq.quantization.gptq] duration: 2.3421690464019775\n",
      "2023-11-09 07:24:31 INFO [auto_gptq.quantization.gptq] avg loss: 4.158407211303711\n",
      "2023-11-09 07:24:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 16/32...\n",
      "2023-11-09 07:24:32 INFO [auto_gptq.quantization.gptq] duration: 1.853118658065796\n",
      "2023-11-09 07:24:32 INFO [auto_gptq.quantization.gptq] avg loss: 10.249532699584961\n",
      "2023-11-09 07:24:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 16/32...\n",
      "2023-11-09 07:24:34 INFO [auto_gptq.quantization.gptq] duration: 1.8384826183319092\n",
      "2023-11-09 07:24:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.12464628368616104\n",
      "2023-11-09 07:24:34 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 16/32...\n",
      "2023-11-09 07:24:36 INFO [auto_gptq.quantization.gptq] duration: 2.1270363330841064\n",
      "2023-11-09 07:24:36 INFO [auto_gptq.quantization.gptq] avg loss: 5.503767490386963\n",
      "2023-11-09 07:24:36 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 16/32...\n",
      "2023-11-09 07:24:39 INFO [auto_gptq.quantization.gptq] duration: 2.0620672702789307\n",
      "2023-11-09 07:24:39 INFO [auto_gptq.quantization.gptq] avg loss: 5.799918174743652\n",
      "2023-11-09 07:24:39 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 16/32...\n",
      "2023-11-09 07:24:42 INFO [auto_gptq.quantization.gptq] duration: 3.829756259918213\n",
      "2023-11-09 07:24:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.3434807062149048\n",
      "2023-11-09 07:24:43 INFO [auto_gptq.modeling._base] Start quantizing layer 17/32\n",
      "2023-11-09 07:24:43 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 17/32...\n",
      "2023-11-09 07:24:46 INFO [auto_gptq.quantization.gptq] duration: 2.4821817874908447\n",
      "2023-11-09 07:24:46 INFO [auto_gptq.quantization.gptq] avg loss: 10.423195838928223\n",
      "2023-11-09 07:24:46 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 17/32...\n",
      "2023-11-09 07:24:47 INFO [auto_gptq.quantization.gptq] duration: 1.458489179611206\n",
      "2023-11-09 07:24:47 INFO [auto_gptq.quantization.gptq] avg loss: 4.469611167907715\n",
      "2023-11-09 07:24:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 17/32...\n",
      "2023-11-09 07:24:48 INFO [auto_gptq.quantization.gptq] duration: 1.3256111145019531\n",
      "2023-11-09 07:24:48 INFO [auto_gptq.quantization.gptq] avg loss: 10.049285888671875\n",
      "2023-11-09 07:24:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 17/32...\n",
      "2023-11-09 07:24:50 INFO [auto_gptq.quantization.gptq] duration: 1.333465814590454\n",
      "2023-11-09 07:24:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.19552400708198547\n",
      "2023-11-09 07:24:50 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 17/32...\n",
      "2023-11-09 07:24:51 INFO [auto_gptq.quantization.gptq] duration: 1.3868205547332764\n",
      "2023-11-09 07:24:51 INFO [auto_gptq.quantization.gptq] avg loss: 6.176978588104248\n",
      "2023-11-09 07:24:51 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 17/32...\n",
      "2023-11-09 07:24:53 INFO [auto_gptq.quantization.gptq] duration: 1.4252524375915527\n",
      "2023-11-09 07:24:53 INFO [auto_gptq.quantization.gptq] avg loss: 6.570391654968262\n",
      "2023-11-09 07:24:53 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 17/32...\n",
      "2023-11-09 07:24:57 INFO [auto_gptq.quantization.gptq] duration: 4.417362928390503\n",
      "2023-11-09 07:24:57 INFO [auto_gptq.quantization.gptq] avg loss: 0.6511077880859375\n",
      "2023-11-09 07:24:57 INFO [auto_gptq.modeling._base] Start quantizing layer 18/32\n",
      "2023-11-09 07:24:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 18/32...\n",
      "2023-11-09 07:24:59 INFO [auto_gptq.quantization.gptq] duration: 1.344090223312378\n",
      "2023-11-09 07:24:59 INFO [auto_gptq.quantization.gptq] avg loss: 9.424493789672852\n",
      "2023-11-09 07:24:59 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 18/32...\n",
      "2023-11-09 07:25:00 INFO [auto_gptq.quantization.gptq] duration: 1.32850980758667\n",
      "2023-11-09 07:25:00 INFO [auto_gptq.quantization.gptq] avg loss: 4.142794132232666\n",
      "2023-11-09 07:25:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 18/32...\n",
      "2023-11-09 07:25:02 INFO [auto_gptq.quantization.gptq] duration: 2.083908796310425\n",
      "2023-11-09 07:25:02 INFO [auto_gptq.quantization.gptq] avg loss: 9.23803997039795\n",
      "2023-11-09 07:25:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 18/32...\n",
      "2023-11-09 07:25:04 INFO [auto_gptq.quantization.gptq] duration: 1.8991107940673828\n",
      "2023-11-09 07:25:04 INFO [auto_gptq.quantization.gptq] avg loss: 0.15210787951946259\n",
      "2023-11-09 07:25:04 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 18/32...\n",
      "2023-11-09 07:25:06 INFO [auto_gptq.quantization.gptq] duration: 1.3531544208526611\n",
      "2023-11-09 07:25:06 INFO [auto_gptq.quantization.gptq] avg loss: 6.460169792175293\n",
      "2023-11-09 07:25:06 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 18/32...\n",
      "2023-11-09 07:25:07 INFO [auto_gptq.quantization.gptq] duration: 1.3465735912322998\n",
      "2023-11-09 07:25:07 INFO [auto_gptq.quantization.gptq] avg loss: 7.086101055145264\n",
      "2023-11-09 07:25:07 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 18/32...\n",
      "2023-11-09 07:25:11 INFO [auto_gptq.quantization.gptq] duration: 3.887491464614868\n",
      "2023-11-09 07:25:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.506402313709259\n",
      "2023-11-09 07:25:11 INFO [auto_gptq.modeling._base] Start quantizing layer 19/32\n",
      "2023-11-09 07:25:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 19/32...\n",
      "2023-11-09 07:25:14 INFO [auto_gptq.quantization.gptq] duration: 2.107473611831665\n",
      "2023-11-09 07:25:14 INFO [auto_gptq.quantization.gptq] avg loss: 9.726181030273438\n",
      "2023-11-09 07:25:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 19/32...\n",
      "2023-11-09 07:25:15 INFO [auto_gptq.quantization.gptq] duration: 1.5075511932373047\n",
      "2023-11-09 07:25:15 INFO [auto_gptq.quantization.gptq] avg loss: 4.932393550872803\n",
      "2023-11-09 07:25:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 19/32...\n",
      "2023-11-09 07:25:17 INFO [auto_gptq.quantization.gptq] duration: 1.3958542346954346\n",
      "2023-11-09 07:25:17 INFO [auto_gptq.quantization.gptq] avg loss: 9.684808731079102\n",
      "2023-11-09 07:25:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 19/32...\n",
      "2023-11-09 07:25:18 INFO [auto_gptq.quantization.gptq] duration: 1.3736488819122314\n",
      "2023-11-09 07:25:18 INFO [auto_gptq.quantization.gptq] avg loss: 0.15747903287410736\n",
      "2023-11-09 07:25:18 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 19/32...\n",
      "2023-11-09 07:25:19 INFO [auto_gptq.quantization.gptq] duration: 1.4434864521026611\n",
      "2023-11-09 07:25:19 INFO [auto_gptq.quantization.gptq] avg loss: 7.0951948165893555\n",
      "2023-11-09 07:25:19 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 19/32...\n",
      "2023-11-09 07:25:21 INFO [auto_gptq.quantization.gptq] duration: 1.7900621891021729\n",
      "2023-11-09 07:25:21 INFO [auto_gptq.quantization.gptq] avg loss: 7.971164703369141\n",
      "2023-11-09 07:25:21 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 19/32...\n",
      "2023-11-09 07:25:26 INFO [auto_gptq.quantization.gptq] duration: 4.918540000915527\n",
      "2023-11-09 07:25:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.6841573715209961\n",
      "2023-11-09 07:25:27 INFO [auto_gptq.modeling._base] Start quantizing layer 20/32\n",
      "2023-11-09 07:25:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 20/32...\n",
      "2023-11-09 07:25:28 INFO [auto_gptq.quantization.gptq] duration: 1.4968128204345703\n",
      "2023-11-09 07:25:28 INFO [auto_gptq.quantization.gptq] avg loss: 9.215217590332031\n",
      "2023-11-09 07:25:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 20/32...\n",
      "2023-11-09 07:25:30 INFO [auto_gptq.quantization.gptq] duration: 1.7571358680725098\n",
      "2023-11-09 07:25:30 INFO [auto_gptq.quantization.gptq] avg loss: 4.853650093078613\n",
      "2023-11-09 07:25:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 20/32...\n",
      "2023-11-09 07:25:32 INFO [auto_gptq.quantization.gptq] duration: 1.7458155155181885\n",
      "2023-11-09 07:25:32 INFO [auto_gptq.quantization.gptq] avg loss: 9.27448558807373\n",
      "2023-11-09 07:25:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 20/32...\n",
      "2023-11-09 07:25:34 INFO [auto_gptq.quantization.gptq] duration: 1.7350552082061768\n",
      "2023-11-09 07:25:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.15927469730377197\n",
      "2023-11-09 07:25:34 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 20/32...\n",
      "2023-11-09 07:25:35 INFO [auto_gptq.quantization.gptq] duration: 1.4084129333496094\n",
      "2023-11-09 07:25:35 INFO [auto_gptq.quantization.gptq] avg loss: 8.126526832580566\n",
      "2023-11-09 07:25:35 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 20/32...\n",
      "2023-11-09 07:25:37 INFO [auto_gptq.quantization.gptq] duration: 1.372638463973999\n",
      "2023-11-09 07:25:37 INFO [auto_gptq.quantization.gptq] avg loss: 9.207429885864258\n",
      "2023-11-09 07:25:37 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 20/32...\n",
      "2023-11-09 07:25:40 INFO [auto_gptq.quantization.gptq] duration: 3.8785579204559326\n",
      "2023-11-09 07:25:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.9131320714950562\n",
      "2023-11-09 07:25:41 INFO [auto_gptq.modeling._base] Start quantizing layer 21/32\n",
      "2023-11-09 07:25:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 21/32...\n",
      "2023-11-09 07:25:42 INFO [auto_gptq.quantization.gptq] duration: 1.3715202808380127\n",
      "2023-11-09 07:25:42 INFO [auto_gptq.quantization.gptq] avg loss: 9.436866760253906\n",
      "2023-11-09 07:25:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 21/32...\n",
      "2023-11-09 07:25:44 INFO [auto_gptq.quantization.gptq] duration: 1.3586325645446777\n",
      "2023-11-09 07:25:44 INFO [auto_gptq.quantization.gptq] avg loss: 4.993273735046387\n",
      "2023-11-09 07:25:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 21/32...\n",
      "2023-11-09 07:25:45 INFO [auto_gptq.quantization.gptq] duration: 1.360933542251587\n",
      "2023-11-09 07:25:45 INFO [auto_gptq.quantization.gptq] avg loss: 9.6458740234375\n",
      "2023-11-09 07:25:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 21/32...\n",
      "2023-11-09 07:25:47 INFO [auto_gptq.quantization.gptq] duration: 1.7707021236419678\n",
      "2023-11-09 07:25:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.12479414790868759\n",
      "2023-11-09 07:25:47 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 21/32...\n",
      "2023-11-09 07:25:49 INFO [auto_gptq.quantization.gptq] duration: 1.690354585647583\n",
      "2023-11-09 07:25:49 INFO [auto_gptq.quantization.gptq] avg loss: 8.161554336547852\n",
      "2023-11-09 07:25:49 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 21/32...\n",
      "2023-11-09 07:25:50 INFO [auto_gptq.quantization.gptq] duration: 1.378983736038208\n",
      "2023-11-09 07:25:50 INFO [auto_gptq.quantization.gptq] avg loss: 9.31088638305664\n",
      "2023-11-09 07:25:50 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 21/32...\n",
      "2023-11-09 07:25:54 INFO [auto_gptq.quantization.gptq] duration: 3.860898971557617\n",
      "2023-11-09 07:25:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.7780497074127197\n",
      "2023-11-09 07:25:54 INFO [auto_gptq.modeling._base] Start quantizing layer 22/32\n",
      "2023-11-09 07:25:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 22/32...\n",
      "2023-11-09 07:25:56 INFO [auto_gptq.quantization.gptq] duration: 1.3635945320129395\n",
      "2023-11-09 07:25:56 INFO [auto_gptq.quantization.gptq] avg loss: 9.760091781616211\n",
      "2023-11-09 07:25:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 22/32...\n",
      "2023-11-09 07:25:57 INFO [auto_gptq.quantization.gptq] duration: 1.5260834693908691\n",
      "2023-11-09 07:25:57 INFO [auto_gptq.quantization.gptq] avg loss: 5.793943881988525\n",
      "2023-11-09 07:25:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 22/32...\n",
      "2023-11-09 07:26:00 INFO [auto_gptq.quantization.gptq] duration: 2.073173761367798\n",
      "2023-11-09 07:26:00 INFO [auto_gptq.quantization.gptq] avg loss: 10.136163711547852\n",
      "2023-11-09 07:26:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 22/32...\n",
      "2023-11-09 07:26:01 INFO [auto_gptq.quantization.gptq] duration: 1.6567504405975342\n",
      "2023-11-09 07:26:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.14293986558914185\n",
      "2023-11-09 07:26:01 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 22/32...\n",
      "2023-11-09 07:26:03 INFO [auto_gptq.quantization.gptq] duration: 1.7212541103363037\n",
      "2023-11-09 07:26:03 INFO [auto_gptq.quantization.gptq] avg loss: 8.79737663269043\n",
      "2023-11-09 07:26:03 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 22/32...\n",
      "2023-11-09 07:26:04 INFO [auto_gptq.quantization.gptq] duration: 1.4049129486083984\n",
      "2023-11-09 07:26:04 INFO [auto_gptq.quantization.gptq] avg loss: 10.219206809997559\n",
      "2023-11-09 07:26:04 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 22/32...\n",
      "2023-11-09 07:26:08 INFO [auto_gptq.quantization.gptq] duration: 3.9066758155822754\n",
      "2023-11-09 07:26:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.8131580352783203\n",
      "2023-11-09 07:26:09 INFO [auto_gptq.modeling._base] Start quantizing layer 23/32\n",
      "2023-11-09 07:26:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 23/32...\n",
      "2023-11-09 07:26:11 INFO [auto_gptq.quantization.gptq] duration: 1.747673511505127\n",
      "2023-11-09 07:26:11 INFO [auto_gptq.quantization.gptq] avg loss: 10.552531242370605\n",
      "2023-11-09 07:26:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 23/32...\n",
      "2023-11-09 07:26:12 INFO [auto_gptq.quantization.gptq] duration: 1.4135644435882568\n",
      "2023-11-09 07:26:12 INFO [auto_gptq.quantization.gptq] avg loss: 6.073057651519775\n",
      "2023-11-09 07:26:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 23/32...\n",
      "2023-11-09 07:26:13 INFO [auto_gptq.quantization.gptq] duration: 1.3577754497528076\n",
      "2023-11-09 07:26:13 INFO [auto_gptq.quantization.gptq] avg loss: 10.775885581970215\n",
      "2023-11-09 07:26:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 23/32...\n",
      "2023-11-09 07:26:15 INFO [auto_gptq.quantization.gptq] duration: 1.363846778869629\n",
      "2023-11-09 07:26:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.14105549454689026\n",
      "2023-11-09 07:26:15 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 23/32...\n",
      "2023-11-09 07:26:16 INFO [auto_gptq.quantization.gptq] duration: 1.4300587177276611\n",
      "2023-11-09 07:26:16 INFO [auto_gptq.quantization.gptq] avg loss: 9.39703369140625\n",
      "2023-11-09 07:26:16 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 23/32...\n",
      "2023-11-09 07:26:18 INFO [auto_gptq.quantization.gptq] duration: 1.4040846824645996\n",
      "2023-11-09 07:26:18 INFO [auto_gptq.quantization.gptq] avg loss: 11.03775405883789\n",
      "2023-11-09 07:26:18 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 23/32...\n",
      "2023-11-09 07:26:22 INFO [auto_gptq.quantization.gptq] duration: 3.88847279548645\n",
      "2023-11-09 07:26:22 INFO [auto_gptq.quantization.gptq] avg loss: 1.0247929096221924\n",
      "2023-11-09 07:26:22 INFO [auto_gptq.modeling._base] Start quantizing layer 24/32\n",
      "2023-11-09 07:26:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 24/32...\n",
      "2023-11-09 07:26:24 INFO [auto_gptq.quantization.gptq] duration: 1.6001574993133545\n",
      "2023-11-09 07:26:24 INFO [auto_gptq.quantization.gptq] avg loss: 11.351645469665527\n",
      "2023-11-09 07:26:24 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 24/32...\n",
      "2023-11-09 07:26:25 INFO [auto_gptq.quantization.gptq] duration: 1.3720238208770752\n",
      "2023-11-09 07:26:25 INFO [auto_gptq.quantization.gptq] avg loss: 7.399351119995117\n",
      "2023-11-09 07:26:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 24/32...\n",
      "2023-11-09 07:26:27 INFO [auto_gptq.quantization.gptq] duration: 1.3332455158233643\n",
      "2023-11-09 07:26:27 INFO [auto_gptq.quantization.gptq] avg loss: 11.747734069824219\n",
      "2023-11-09 07:26:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 24/32...\n",
      "2023-11-09 07:26:28 INFO [auto_gptq.quantization.gptq] duration: 1.407735824584961\n",
      "2023-11-09 07:26:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.12480777502059937\n",
      "2023-11-09 07:26:28 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 24/32...\n",
      "2023-11-09 07:26:29 INFO [auto_gptq.quantization.gptq] duration: 1.3881652355194092\n",
      "2023-11-09 07:26:29 INFO [auto_gptq.quantization.gptq] avg loss: 10.222375869750977\n",
      "2023-11-09 07:26:29 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 24/32...\n",
      "2023-11-09 07:26:31 INFO [auto_gptq.quantization.gptq] duration: 1.631211519241333\n",
      "2023-11-09 07:26:31 INFO [auto_gptq.quantization.gptq] avg loss: 11.886028289794922\n",
      "2023-11-09 07:26:31 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 24/32...\n",
      "2023-11-09 07:26:35 INFO [auto_gptq.quantization.gptq] duration: 3.86909556388855\n",
      "2023-11-09 07:26:35 INFO [auto_gptq.quantization.gptq] avg loss: 1.0829944610595703\n",
      "2023-11-09 07:26:35 INFO [auto_gptq.modeling._base] Start quantizing layer 25/32\n",
      "2023-11-09 07:26:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 25/32...\n",
      "2023-11-09 07:26:37 INFO [auto_gptq.quantization.gptq] duration: 1.389803409576416\n",
      "2023-11-09 07:26:37 INFO [auto_gptq.quantization.gptq] avg loss: 10.858413696289062\n",
      "2023-11-09 07:26:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 25/32...\n",
      "2023-11-09 07:26:38 INFO [auto_gptq.quantization.gptq] duration: 1.3542828559875488\n",
      "2023-11-09 07:26:38 INFO [auto_gptq.quantization.gptq] avg loss: 7.2176666259765625\n",
      "2023-11-09 07:26:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 25/32...\n",
      "2023-11-09 07:26:40 INFO [auto_gptq.quantization.gptq] duration: 1.3957440853118896\n",
      "2023-11-09 07:26:40 INFO [auto_gptq.quantization.gptq] avg loss: 11.06541919708252\n",
      "2023-11-09 07:26:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 25/32...\n",
      "2023-11-09 07:26:41 INFO [auto_gptq.quantization.gptq] duration: 1.350794792175293\n",
      "2023-11-09 07:26:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.24426156282424927\n",
      "2023-11-09 07:26:41 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 25/32...\n",
      "2023-11-09 07:26:42 INFO [auto_gptq.quantization.gptq] duration: 1.4004161357879639\n",
      "2023-11-09 07:26:42 INFO [auto_gptq.quantization.gptq] avg loss: 10.697481155395508\n",
      "2023-11-09 07:26:43 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 25/32...\n",
      "2023-11-09 07:26:44 INFO [auto_gptq.quantization.gptq] duration: 1.3974897861480713\n",
      "2023-11-09 07:26:44 INFO [auto_gptq.quantization.gptq] avg loss: 12.43014144897461\n",
      "2023-11-09 07:26:44 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 25/32...\n",
      "2023-11-09 07:26:48 INFO [auto_gptq.quantization.gptq] duration: 3.875406503677368\n",
      "2023-11-09 07:26:48 INFO [auto_gptq.quantization.gptq] avg loss: 1.0978896617889404\n",
      "2023-11-09 07:26:48 INFO [auto_gptq.modeling._base] Start quantizing layer 26/32\n",
      "2023-11-09 07:26:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 26/32...\n",
      "2023-11-09 07:26:50 INFO [auto_gptq.quantization.gptq] duration: 1.3688209056854248\n",
      "2023-11-09 07:26:50 INFO [auto_gptq.quantization.gptq] avg loss: 12.108329772949219\n",
      "2023-11-09 07:26:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 26/32...\n",
      "2023-11-09 07:26:51 INFO [auto_gptq.quantization.gptq] duration: 1.3750197887420654\n",
      "2023-11-09 07:26:51 INFO [auto_gptq.quantization.gptq] avg loss: 8.695610046386719\n",
      "2023-11-09 07:26:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 26/32...\n",
      "2023-11-09 07:26:52 INFO [auto_gptq.quantization.gptq] duration: 1.3596246242523193\n",
      "2023-11-09 07:26:52 INFO [auto_gptq.quantization.gptq] avg loss: 12.677192687988281\n",
      "2023-11-09 07:26:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 26/32...\n",
      "2023-11-09 07:26:54 INFO [auto_gptq.quantization.gptq] duration: 1.3737587928771973\n",
      "2023-11-09 07:26:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.29011601209640503\n",
      "2023-11-09 07:26:54 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 26/32...\n",
      "2023-11-09 07:26:55 INFO [auto_gptq.quantization.gptq] duration: 1.37705397605896\n",
      "2023-11-09 07:26:55 INFO [auto_gptq.quantization.gptq] avg loss: 11.73712158203125\n",
      "2023-11-09 07:26:55 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 26/32...\n",
      "2023-11-09 07:26:57 INFO [auto_gptq.quantization.gptq] duration: 1.3984827995300293\n",
      "2023-11-09 07:26:57 INFO [auto_gptq.quantization.gptq] avg loss: 13.596118927001953\n",
      "2023-11-09 07:26:57 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 26/32...\n",
      "2023-11-09 07:27:01 INFO [auto_gptq.quantization.gptq] duration: 3.9084763526916504\n",
      "2023-11-09 07:27:01 INFO [auto_gptq.quantization.gptq] avg loss: 1.3379638195037842\n",
      "2023-11-09 07:27:01 INFO [auto_gptq.modeling._base] Start quantizing layer 27/32\n",
      "2023-11-09 07:27:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 27/32...\n",
      "2023-11-09 07:27:03 INFO [auto_gptq.quantization.gptq] duration: 1.9907503128051758\n",
      "2023-11-09 07:27:03 INFO [auto_gptq.quantization.gptq] avg loss: 11.630035400390625\n",
      "2023-11-09 07:27:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 27/32...\n",
      "2023-11-09 07:27:05 INFO [auto_gptq.quantization.gptq] duration: 1.3650505542755127\n",
      "2023-11-09 07:27:05 INFO [auto_gptq.quantization.gptq] avg loss: 8.730042457580566\n",
      "2023-11-09 07:27:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 27/32...\n",
      "2023-11-09 07:27:06 INFO [auto_gptq.quantization.gptq] duration: 1.3805837631225586\n",
      "2023-11-09 07:27:06 INFO [auto_gptq.quantization.gptq] avg loss: 12.387718200683594\n",
      "2023-11-09 07:27:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 27/32...\n",
      "2023-11-09 07:27:07 INFO [auto_gptq.quantization.gptq] duration: 1.3541877269744873\n",
      "2023-11-09 07:27:07 INFO [auto_gptq.quantization.gptq] avg loss: 0.434187114238739\n",
      "2023-11-09 07:27:07 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 27/32...\n",
      "2023-11-09 07:27:09 INFO [auto_gptq.quantization.gptq] duration: 1.4281363487243652\n",
      "2023-11-09 07:27:09 INFO [auto_gptq.quantization.gptq] avg loss: 12.783563613891602\n",
      "2023-11-09 07:27:09 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 27/32...\n",
      "2023-11-09 07:27:10 INFO [auto_gptq.quantization.gptq] duration: 1.3892662525177002\n",
      "2023-11-09 07:27:10 INFO [auto_gptq.quantization.gptq] avg loss: 14.699621200561523\n",
      "2023-11-09 07:27:10 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 27/32...\n",
      "2023-11-09 07:27:14 INFO [auto_gptq.quantization.gptq] duration: 3.9429540634155273\n",
      "2023-11-09 07:27:14 INFO [auto_gptq.quantization.gptq] avg loss: 1.462045669555664\n",
      "2023-11-09 07:27:15 INFO [auto_gptq.modeling._base] Start quantizing layer 28/32\n",
      "2023-11-09 07:27:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 28/32...\n",
      "2023-11-09 07:27:16 INFO [auto_gptq.quantization.gptq] duration: 1.3552980422973633\n",
      "2023-11-09 07:27:16 INFO [auto_gptq.quantization.gptq] avg loss: 12.265559196472168\n",
      "2023-11-09 07:27:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 28/32...\n",
      "2023-11-09 07:27:18 INFO [auto_gptq.quantization.gptq] duration: 1.3828532695770264\n",
      "2023-11-09 07:27:18 INFO [auto_gptq.quantization.gptq] avg loss: 8.807798385620117\n",
      "2023-11-09 07:27:18 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 28/32...\n",
      "2023-11-09 07:27:19 INFO [auto_gptq.quantization.gptq] duration: 1.413907766342163\n",
      "2023-11-09 07:27:19 INFO [auto_gptq.quantization.gptq] avg loss: 12.986788749694824\n",
      "2023-11-09 07:27:19 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 28/32...\n",
      "2023-11-09 07:27:20 INFO [auto_gptq.quantization.gptq] duration: 1.3462402820587158\n",
      "2023-11-09 07:27:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.36005860567092896\n",
      "2023-11-09 07:27:20 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 28/32...\n",
      "2023-11-09 07:27:22 INFO [auto_gptq.quantization.gptq] duration: 1.379235029220581\n",
      "2023-11-09 07:27:22 INFO [auto_gptq.quantization.gptq] avg loss: 13.894594192504883\n",
      "2023-11-09 07:27:22 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 28/32...\n",
      "2023-11-09 07:27:23 INFO [auto_gptq.quantization.gptq] duration: 1.3985795974731445\n",
      "2023-11-09 07:27:23 INFO [auto_gptq.quantization.gptq] avg loss: 15.912923812866211\n",
      "2023-11-09 07:27:23 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 28/32...\n",
      "2023-11-09 07:27:27 INFO [auto_gptq.quantization.gptq] duration: 3.9467127323150635\n",
      "2023-11-09 07:27:27 INFO [auto_gptq.quantization.gptq] avg loss: 1.6159554719924927\n",
      "2023-11-09 07:27:28 INFO [auto_gptq.modeling._base] Start quantizing layer 29/32\n",
      "2023-11-09 07:27:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 29/32...\n",
      "2023-11-09 07:27:29 INFO [auto_gptq.quantization.gptq] duration: 1.3857181072235107\n",
      "2023-11-09 07:27:29 INFO [auto_gptq.quantization.gptq] avg loss: 12.17360782623291\n",
      "2023-11-09 07:27:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 29/32...\n",
      "2023-11-09 07:27:31 INFO [auto_gptq.quantization.gptq] duration: 1.3527777194976807\n",
      "2023-11-09 07:27:31 INFO [auto_gptq.quantization.gptq] avg loss: 9.788594245910645\n",
      "2023-11-09 07:27:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 29/32...\n",
      "2023-11-09 07:27:32 INFO [auto_gptq.quantization.gptq] duration: 1.361574649810791\n",
      "2023-11-09 07:27:32 INFO [auto_gptq.quantization.gptq] avg loss: 13.201635360717773\n",
      "2023-11-09 07:27:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 29/32...\n",
      "2023-11-09 07:27:33 INFO [auto_gptq.quantization.gptq] duration: 1.3627424240112305\n",
      "2023-11-09 07:27:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.44292598962783813\n",
      "2023-11-09 07:27:33 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 29/32...\n",
      "2023-11-09 07:27:35 INFO [auto_gptq.quantization.gptq] duration: 1.4187886714935303\n",
      "2023-11-09 07:27:35 INFO [auto_gptq.quantization.gptq] avg loss: 14.989107131958008\n",
      "2023-11-09 07:27:35 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 29/32...\n",
      "2023-11-09 07:27:36 INFO [auto_gptq.quantization.gptq] duration: 1.3785781860351562\n",
      "2023-11-09 07:27:36 INFO [auto_gptq.quantization.gptq] avg loss: 16.643585205078125\n",
      "2023-11-09 07:27:36 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 29/32...\n",
      "2023-11-09 07:27:40 INFO [auto_gptq.quantization.gptq] duration: 3.8914356231689453\n",
      "2023-11-09 07:27:40 INFO [auto_gptq.quantization.gptq] avg loss: 2.1404776573181152\n",
      "2023-11-09 07:27:40 INFO [auto_gptq.modeling._base] Start quantizing layer 30/32\n",
      "2023-11-09 07:27:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 30/32...\n",
      "2023-11-09 07:27:42 INFO [auto_gptq.quantization.gptq] duration: 1.3563222885131836\n",
      "2023-11-09 07:27:42 INFO [auto_gptq.quantization.gptq] avg loss: 10.941459655761719\n",
      "2023-11-09 07:27:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 30/32...\n",
      "2023-11-09 07:27:44 INFO [auto_gptq.quantization.gptq] duration: 1.445540428161621\n",
      "2023-11-09 07:27:44 INFO [auto_gptq.quantization.gptq] avg loss: 9.242929458618164\n",
      "2023-11-09 07:27:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 30/32...\n",
      "2023-11-09 07:27:45 INFO [auto_gptq.quantization.gptq] duration: 1.7066218852996826\n",
      "2023-11-09 07:27:45 INFO [auto_gptq.quantization.gptq] avg loss: 11.706439971923828\n",
      "2023-11-09 07:27:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 30/32...\n",
      "2023-11-09 07:27:47 INFO [auto_gptq.quantization.gptq] duration: 1.6407470703125\n",
      "2023-11-09 07:27:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.46650028228759766\n",
      "2023-11-09 07:27:47 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 30/32...\n",
      "2023-11-09 07:27:48 INFO [auto_gptq.quantization.gptq] duration: 1.3832628726959229\n",
      "2023-11-09 07:27:48 INFO [auto_gptq.quantization.gptq] avg loss: 16.123294830322266\n",
      "2023-11-09 07:27:48 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 30/32...\n",
      "2023-11-09 07:27:50 INFO [auto_gptq.quantization.gptq] duration: 1.3729252815246582\n",
      "2023-11-09 07:27:50 INFO [auto_gptq.quantization.gptq] avg loss: 17.710817337036133\n",
      "2023-11-09 07:27:50 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 30/32...\n",
      "2023-11-09 07:27:54 INFO [auto_gptq.quantization.gptq] duration: 4.098554372787476\n",
      "2023-11-09 07:27:54 INFO [auto_gptq.quantization.gptq] avg loss: 4.639873504638672\n",
      "2023-11-09 07:27:54 INFO [auto_gptq.modeling._base] Start quantizing layer 31/32\n",
      "2023-11-09 07:27:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 31/32...\n",
      "2023-11-09 07:27:56 INFO [auto_gptq.quantization.gptq] duration: 1.356893539428711\n",
      "2023-11-09 07:27:56 INFO [auto_gptq.quantization.gptq] avg loss: 11.581430435180664\n",
      "2023-11-09 07:27:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 31/32...\n",
      "2023-11-09 07:27:57 INFO [auto_gptq.quantization.gptq] duration: 1.3542861938476562\n",
      "2023-11-09 07:27:57 INFO [auto_gptq.quantization.gptq] avg loss: 10.121774673461914\n",
      "2023-11-09 07:27:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 31/32...\n",
      "2023-11-09 07:27:58 INFO [auto_gptq.quantization.gptq] duration: 1.3763737678527832\n",
      "2023-11-09 07:27:58 INFO [auto_gptq.quantization.gptq] avg loss: 13.180875778198242\n",
      "2023-11-09 07:27:59 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 31/32...\n",
      "2023-11-09 07:28:00 INFO [auto_gptq.quantization.gptq] duration: 1.3692684173583984\n",
      "2023-11-09 07:28:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.6051214933395386\n",
      "2023-11-09 07:28:00 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 31/32...\n",
      "2023-11-09 07:28:01 INFO [auto_gptq.quantization.gptq] duration: 1.3940544128417969\n",
      "2023-11-09 07:28:01 INFO [auto_gptq.quantization.gptq] avg loss: 18.138031005859375\n",
      "2023-11-09 07:28:01 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 31/32...\n",
      "2023-11-09 07:28:03 INFO [auto_gptq.quantization.gptq] duration: 1.380030870437622\n",
      "2023-11-09 07:28:03 INFO [auto_gptq.quantization.gptq] avg loss: 20.093215942382812\n",
      "2023-11-09 07:28:03 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 31/32...\n",
      "2023-11-09 07:28:07 INFO [auto_gptq.quantization.gptq] duration: 3.9760799407958984\n",
      "2023-11-09 07:28:07 INFO [auto_gptq.quantization.gptq] avg loss: 1205.5966796875\n",
      "2023-11-09 07:28:07 INFO [auto_gptq.modeling._base] Start quantizing layer 32/32\n",
      "2023-11-09 07:28:07 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 32/32...\n",
      "2023-11-09 07:28:09 INFO [auto_gptq.quantization.gptq] duration: 1.3721973896026611\n",
      "2023-11-09 07:28:09 INFO [auto_gptq.quantization.gptq] avg loss: 9.034016609191895\n",
      "2023-11-09 07:28:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 32/32...\n",
      "2023-11-09 07:28:10 INFO [auto_gptq.quantization.gptq] duration: 1.4381883144378662\n",
      "2023-11-09 07:28:10 INFO [auto_gptq.quantization.gptq] avg loss: 6.127747058868408\n",
      "2023-11-09 07:28:10 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 32/32...\n",
      "2023-11-09 07:28:11 INFO [auto_gptq.quantization.gptq] duration: 1.3991706371307373\n",
      "2023-11-09 07:28:11 INFO [auto_gptq.quantization.gptq] avg loss: 8.749870300292969\n",
      "2023-11-09 07:28:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 32/32...\n",
      "2023-11-09 07:28:13 INFO [auto_gptq.quantization.gptq] duration: 1.3620665073394775\n",
      "2023-11-09 07:28:13 INFO [auto_gptq.quantization.gptq] avg loss: 1.3836696147918701\n",
      "2023-11-09 07:28:13 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 32/32...\n",
      "2023-11-09 07:28:14 INFO [auto_gptq.quantization.gptq] duration: 1.611293077468872\n",
      "2023-11-09 07:28:14 INFO [auto_gptq.quantization.gptq] avg loss: 15.09185791015625\n",
      "2023-11-09 07:28:15 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 32/32...\n",
      "2023-11-09 07:28:16 INFO [auto_gptq.quantization.gptq] duration: 1.3761816024780273\n",
      "2023-11-09 07:28:16 INFO [auto_gptq.quantization.gptq] avg loss: 16.921741485595703\n",
      "2023-11-09 07:28:16 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 32/32...\n",
      "2023-11-09 07:28:20 INFO [auto_gptq.quantization.gptq] duration: 3.8663101196289062\n",
      "2023-11-09 07:28:20 INFO [auto_gptq.quantization.gptq] avg loss: 17.717300415039062\n",
      "2023-11-09 07:28:20 INFO [auto_gptq.modeling._utils] Packing model...\n",
      "2023-11-09 07:28:22 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.k_proj\n",
      "2023-11-09 07:28:32 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.o_proj\n",
      "2023-11-09 07:28:42 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.q_proj\n",
      "2023-11-09 07:28:51 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.v_proj\n",
      "2023-11-09 07:28:59 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.down_proj\n",
      "2023-11-09 07:29:18 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.gate_proj\n",
      "2023-11-09 07:29:24 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.up_proj\n",
      "2023-11-09 07:29:27 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.k_proj\n",
      "2023-11-09 07:29:28 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.o_proj\n",
      "2023-11-09 07:29:29 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.q_proj\n",
      "2023-11-09 07:29:30 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.v_proj\n",
      "2023-11-09 07:29:31 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.down_proj\n",
      "2023-11-09 07:29:34 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.gate_proj\n",
      "2023-11-09 07:29:37 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.up_proj\n",
      "2023-11-09 07:29:39 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.k_proj\n",
      "2023-11-09 07:29:40 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.o_proj\n",
      "2023-11-09 07:29:41 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.q_proj\n",
      "2023-11-09 07:29:42 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.v_proj\n",
      "2023-11-09 07:29:43 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.down_proj\n",
      "2023-11-09 07:29:45 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.gate_proj\n",
      "2023-11-09 07:29:48 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.up_proj\n",
      "2023-11-09 07:29:51 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.k_proj\n",
      "2023-11-09 07:29:52 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.o_proj\n",
      "2023-11-09 07:29:53 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.q_proj\n",
      "2023-11-09 07:29:53 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.v_proj\n",
      "2023-11-09 07:29:54 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.down_proj\n",
      "2023-11-09 07:29:57 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.gate_proj\n",
      "2023-11-09 07:29:59 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.up_proj\n",
      "2023-11-09 07:30:01 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.k_proj\n",
      "2023-11-09 07:30:02 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.o_proj\n",
      "2023-11-09 07:30:03 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.q_proj\n",
      "2023-11-09 07:30:04 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.v_proj\n",
      "2023-11-09 07:30:05 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.down_proj\n",
      "2023-11-09 07:30:08 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.gate_proj\n",
      "2023-11-09 07:30:11 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.up_proj\n",
      "2023-11-09 07:30:13 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.k_proj\n",
      "2023-11-09 07:30:14 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.o_proj\n",
      "2023-11-09 07:30:15 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.q_proj\n",
      "2023-11-09 07:30:16 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.v_proj\n",
      "2023-11-09 07:30:17 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.down_proj\n",
      "2023-11-09 07:30:19 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.gate_proj\n",
      "2023-11-09 07:30:22 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.up_proj\n",
      "2023-11-09 07:30:24 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.k_proj\n",
      "2023-11-09 07:30:25 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.o_proj\n",
      "2023-11-09 07:30:26 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.q_proj\n",
      "2023-11-09 07:30:27 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.v_proj\n",
      "2023-11-09 07:30:28 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.down_proj\n",
      "2023-11-09 07:30:30 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.gate_proj\n",
      "2023-11-09 07:30:32 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.up_proj\n",
      "2023-11-09 07:30:35 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.k_proj\n",
      "2023-11-09 07:30:35 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.o_proj\n",
      "2023-11-09 07:30:36 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.q_proj\n",
      "2023-11-09 07:30:37 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.v_proj\n",
      "2023-11-09 07:30:38 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.down_proj\n",
      "2023-11-09 07:30:41 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.gate_proj\n",
      "2023-11-09 07:30:45 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.up_proj\n",
      "2023-11-09 07:30:48 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.k_proj\n",
      "2023-11-09 07:30:49 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.o_proj\n",
      "2023-11-09 07:30:50 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.q_proj\n",
      "2023-11-09 07:30:50 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.v_proj\n",
      "2023-11-09 07:30:51 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.down_proj\n",
      "2023-11-09 07:30:53 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.gate_proj\n",
      "2023-11-09 07:30:56 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.up_proj\n",
      "2023-11-09 07:30:58 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.k_proj\n",
      "2023-11-09 07:30:59 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.o_proj\n",
      "2023-11-09 07:31:00 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.q_proj\n",
      "2023-11-09 07:31:01 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.v_proj\n",
      "2023-11-09 07:31:01 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.down_proj\n",
      "2023-11-09 07:31:04 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.gate_proj\n",
      "2023-11-09 07:31:06 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.up_proj\n",
      "2023-11-09 07:31:08 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.k_proj\n",
      "2023-11-09 07:31:09 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.o_proj\n",
      "2023-11-09 07:31:10 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.q_proj\n",
      "2023-11-09 07:31:11 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.v_proj\n",
      "2023-11-09 07:31:12 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.down_proj\n",
      "2023-11-09 07:31:15 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.gate_proj\n",
      "2023-11-09 07:31:17 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.up_proj\n",
      "2023-11-09 07:31:19 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.k_proj\n",
      "2023-11-09 07:31:20 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.o_proj\n",
      "2023-11-09 07:31:21 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.q_proj\n",
      "2023-11-09 07:31:22 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.v_proj\n",
      "2023-11-09 07:31:23 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.down_proj\n",
      "2023-11-09 07:31:25 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.gate_proj\n",
      "2023-11-09 07:31:27 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.up_proj\n",
      "2023-11-09 07:31:30 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.k_proj\n",
      "2023-11-09 07:31:31 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.o_proj\n",
      "2023-11-09 07:31:32 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.q_proj\n",
      "2023-11-09 07:31:32 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.v_proj\n",
      "2023-11-09 07:31:33 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.down_proj\n",
      "2023-11-09 07:31:36 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.gate_proj\n",
      "2023-11-09 07:31:39 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.up_proj\n",
      "2023-11-09 07:31:41 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.k_proj\n",
      "2023-11-09 07:31:42 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.o_proj\n",
      "2023-11-09 07:31:43 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.q_proj\n",
      "2023-11-09 07:31:44 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.v_proj\n",
      "2023-11-09 07:31:45 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.down_proj\n",
      "2023-11-09 07:31:47 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.gate_proj\n",
      "2023-11-09 07:31:50 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.up_proj\n",
      "2023-11-09 07:31:52 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.k_proj\n",
      "2023-11-09 07:31:53 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.o_proj\n",
      "2023-11-09 07:31:54 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.q_proj\n",
      "2023-11-09 07:31:55 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.v_proj\n",
      "2023-11-09 07:31:56 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.down_proj\n",
      "2023-11-09 07:31:59 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.gate_proj\n",
      "2023-11-09 07:32:01 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.up_proj\n",
      "2023-11-09 07:32:03 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.k_proj\n",
      "2023-11-09 07:32:04 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.o_proj\n",
      "2023-11-09 07:32:05 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.q_proj\n",
      "2023-11-09 07:32:06 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.v_proj\n",
      "2023-11-09 07:32:07 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.down_proj\n",
      "2023-11-09 07:32:09 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.gate_proj\n",
      "2023-11-09 07:32:12 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.up_proj\n",
      "2023-11-09 07:32:15 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.k_proj\n",
      "2023-11-09 07:32:16 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.o_proj\n",
      "2023-11-09 07:32:16 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.q_proj\n",
      "2023-11-09 07:32:17 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.v_proj\n",
      "2023-11-09 07:32:18 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.down_proj\n",
      "2023-11-09 07:32:20 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.gate_proj\n",
      "2023-11-09 07:32:22 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.up_proj\n",
      "2023-11-09 07:32:25 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.k_proj\n",
      "2023-11-09 07:32:25 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.o_proj\n",
      "2023-11-09 07:32:26 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.q_proj\n",
      "2023-11-09 07:32:27 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.v_proj\n",
      "2023-11-09 07:32:28 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.down_proj\n",
      "2023-11-09 07:32:30 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.gate_proj\n",
      "2023-11-09 07:32:33 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.up_proj\n",
      "2023-11-09 07:32:35 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.k_proj\n",
      "2023-11-09 07:32:36 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.o_proj\n",
      "2023-11-09 07:32:37 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.q_proj\n",
      "2023-11-09 07:32:38 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.v_proj\n",
      "2023-11-09 07:32:39 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.down_proj\n",
      "2023-11-09 07:32:42 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.gate_proj\n",
      "2023-11-09 07:32:44 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.up_proj\n",
      "2023-11-09 07:32:46 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.k_proj\n",
      "2023-11-09 07:32:47 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.o_proj\n",
      "2023-11-09 07:32:48 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.q_proj\n",
      "2023-11-09 07:32:49 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.v_proj\n",
      "2023-11-09 07:32:50 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.down_proj\n",
      "2023-11-09 07:32:53 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.gate_proj\n",
      "2023-11-09 07:32:55 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.up_proj\n",
      "2023-11-09 07:32:57 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.k_proj\n",
      "2023-11-09 07:32:58 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.o_proj\n",
      "2023-11-09 07:32:59 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.q_proj\n",
      "2023-11-09 07:33:00 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.v_proj\n",
      "2023-11-09 07:33:01 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.down_proj\n",
      "2023-11-09 07:33:03 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.gate_proj\n",
      "2023-11-09 07:33:05 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.up_proj\n",
      "2023-11-09 07:33:07 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.k_proj\n",
      "2023-11-09 07:33:08 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.o_proj\n",
      "2023-11-09 07:33:09 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.q_proj\n",
      "2023-11-09 07:33:10 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.v_proj\n",
      "2023-11-09 07:33:11 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.down_proj\n",
      "2023-11-09 07:33:13 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.gate_proj\n",
      "2023-11-09 07:33:15 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.up_proj\n",
      "2023-11-09 07:33:17 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.k_proj\n",
      "2023-11-09 07:33:18 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.o_proj\n",
      "2023-11-09 07:33:19 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.q_proj\n",
      "2023-11-09 07:33:20 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.v_proj\n",
      "2023-11-09 07:33:21 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.down_proj\n",
      "2023-11-09 07:33:23 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.gate_proj\n",
      "2023-11-09 07:33:26 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.up_proj\n",
      "2023-11-09 07:33:28 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.k_proj\n",
      "2023-11-09 07:33:29 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.o_proj\n",
      "2023-11-09 07:33:30 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.q_proj\n",
      "2023-11-09 07:33:31 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.v_proj\n",
      "2023-11-09 07:33:32 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.down_proj\n",
      "2023-11-09 07:33:34 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.gate_proj\n",
      "2023-11-09 07:33:37 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.up_proj\n",
      "2023-11-09 07:33:39 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.k_proj\n",
      "2023-11-09 07:33:40 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.o_proj\n",
      "2023-11-09 07:33:41 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.q_proj\n",
      "2023-11-09 07:33:41 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.v_proj\n",
      "2023-11-09 07:33:42 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.down_proj\n",
      "2023-11-09 07:33:45 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.gate_proj\n",
      "2023-11-09 07:33:47 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.up_proj\n",
      "2023-11-09 07:33:49 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.k_proj\n",
      "2023-11-09 07:33:50 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.o_proj\n",
      "2023-11-09 07:33:51 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.q_proj\n",
      "2023-11-09 07:33:52 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.v_proj\n",
      "2023-11-09 07:33:53 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.down_proj\n",
      "2023-11-09 07:33:55 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.gate_proj\n",
      "2023-11-09 07:33:57 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.up_proj\n",
      "2023-11-09 07:33:59 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.k_proj\n",
      "2023-11-09 07:34:00 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.o_proj\n",
      "2023-11-09 07:34:01 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.q_proj\n",
      "2023-11-09 07:34:02 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.v_proj\n",
      "2023-11-09 07:34:03 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.down_proj\n",
      "2023-11-09 07:34:05 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.gate_proj\n",
      "2023-11-09 07:34:07 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.up_proj\n",
      "2023-11-09 07:34:09 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.k_proj\n",
      "2023-11-09 07:34:10 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.o_proj\n",
      "2023-11-09 07:34:11 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.q_proj\n",
      "2023-11-09 07:34:17 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.v_proj\n",
      "2023-11-09 07:34:24 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.down_proj\n",
      "2023-11-09 07:34:42 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.gate_proj\n",
      "2023-11-09 07:34:54 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.up_proj\n",
      "2023-11-09 07:35:05 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.k_proj\n",
      "2023-11-09 07:35:11 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.o_proj\n",
      "2023-11-09 07:35:19 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.q_proj\n",
      "2023-11-09 07:35:23 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.v_proj\n",
      "2023-11-09 07:35:29 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.down_proj\n",
      "2023-11-09 07:35:42 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.gate_proj\n",
      "2023-11-09 07:35:52 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.up_proj\n",
      "2023-11-09 07:36:03 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.k_proj\n",
      "2023-11-09 07:36:12 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.o_proj\n",
      "2023-11-09 07:36:18 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.q_proj\n",
      "2023-11-09 07:36:23 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.v_proj\n",
      "2023-11-09 07:36:31 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.down_proj\n",
      "2023-11-09 07:36:43 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.gate_proj\n",
      "2023-11-09 07:36:55 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.up_proj\n",
      "2023-11-09 07:37:07 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.k_proj\n",
      "2023-11-09 07:37:13 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.o_proj\n",
      "2023-11-09 07:37:20 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.q_proj\n",
      "2023-11-09 07:37:25 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.v_proj\n",
      "2023-11-09 07:37:32 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.down_proj\n",
      "2023-11-09 07:37:47 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.gate_proj\n",
      "2023-11-09 07:37:55 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.up_proj\n",
      "2023-11-09 07:38:05 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.k_proj\n",
      "2023-11-09 07:38:11 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.o_proj\n",
      "2023-11-09 07:38:18 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.q_proj\n",
      "2023-11-09 07:38:28 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.v_proj\n",
      "2023-11-09 07:38:36 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.down_proj\n",
      "2023-11-09 07:38:50 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.gate_proj\n",
      "2023-11-09 07:39:04 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.up_proj\n",
      "2023-11-09 07:39:16 INFO [auto_gptq.modeling._utils] Model packed.\n",
      "2023-11-09 07:39:57 INFO [auto_gptq.modeling._base] lm_head not been quantized, will be ignored when make_quant.\n",
      "2023-11-09 07:40:12 WARNING [auto_gptq.nn_modules.fused_llama_mlp] Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n",
      "/home/qcdong/anaconda3/envs/ll/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> auto_gptq is a Python package for generating text with the GPT-Q model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto-gptq is a powerful tool for generating and optimizing GPT-based models,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline, AutoModelForCausalLM, GPTQConfig\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "## hf also have auto gptq, GPTQConfig\n",
    "# model_id = \"facebook/opt-125m\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/mnt/sdc/yuzhao/model/llm/llama/llama2-7b-chat-hf\"\n",
    "quantized_model_dir = \"/mnt/nas1/models/llama/quantized_models/llama2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
    "examples = [\n",
    "    tokenizer(\n",
    "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "\n",
    "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "model.quantize(examples)\n",
    "\n",
    "# save quantized model\n",
    "model.save_quantized(quantized_model_dir)\n",
    "\n",
    "# save quantized model using safetensors\n",
    "model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
    "\n",
    "# push quantized model to Hugging Face Hub.\n",
    "# to use use_auth_token=True, Login first via huggingface-cli login.\n",
    "# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# alternatively you can save and push at the same time\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# load quantized model to the first GPU\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n",
    "\n",
    "# download quantized model from Hugging Face Hub and load to the first GPU\n",
    "# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=\"cuda:0\", use_safetensors=True, use_triton=False)\n",
    "\n",
    "# inference with model.generate\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
    "\n",
    "# or you can also use pipeline\n",
    "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
