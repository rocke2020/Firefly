{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcdong/anaconda3/envs/ll/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-09 22:20:47 INFO [auto_gptq.modeling._base] lm_head not been quantized, will be ignored when make_quant.\n",
      "2023-11-09 22:21:40 WARNING [auto_gptq.nn_modules.fused_llama_mlp] Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n",
      "/home/qcdong/anaconda3/envs/ll/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "The model 'LlamaGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> auto_gptq is a Python package for generating text with the GPT-Q model\n",
      "auto-gptq is a powerful tool for generating and optimizing GPT-based models,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline, AutoModelForCausalLM, GPTQConfig\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "## hf also have auto gptq, GPTQConfig\n",
    "# model_id = \"facebook/opt-125m\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/mnt/nas1/models/llama/pretrained_weights/llama2-7b-chat-hf\"\n",
    "quantized_model_dir = \"/mnt/nas1/models/llama/quantized_models/llama2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
    "examples = [\n",
    "    tokenizer(\n",
    "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# quantize_config = BaseQuantizeConfig(\n",
    "#     bits=4,  # quantize model to 4-bit\n",
    "#     group_size=128,  # it is recommended to set the value to 128\n",
    "#     desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    "# )\n",
    "\n",
    "# # load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "# model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "\n",
    "# # quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "# model.quantize(examples)\n",
    "\n",
    "# # save quantized model\n",
    "# model.save_quantized(quantized_model_dir)\n",
    "\n",
    "# # save quantized model using safetensors\n",
    "# model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
    "\n",
    "\n",
    "# push quantized model to Hugging Face Hub.\n",
    "# to use use_auth_token=True, Login first via huggingface-cli login.\n",
    "# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# alternatively you can save and push at the same time\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# load quantized model to the first GPU\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n",
    "\n",
    "# download quantized model from Hugging Face Hub and load to the first GPU\n",
    "# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=\"cuda:0\", use_safetensors=True, use_triton=False)\n",
    "\n",
    "# inference with model.generate\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
    "\n",
    "# or you can also use pipeline\n",
    "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
